1. first file to create is .gitlab-ci.yml -> continuous integration gitlab. yaml

2. If error, check the tabs,it may set default is 8 instead 4.

3. the structure is:

stages:
    - stage 1
    - stage 2
    ...

variables: -> list of variables
    buildFileName: "laptop.txt"
    ...

build laptop: -> name of 1st job
    image: node:lts-alpine -> docker's image. It means the alpine dist with lts node installed
    stage: build laptop
    script: -> scripts in order
        - echo "Building laptop..."
        - touch build/$buildFileName
        ...
    artifacts: -> things should be keep after the container is destroyed
        paths:
            - build

test laptop: -> name of 2nd job
    ...

.nothing important: -> add a . to disable job

4. Upload using AWS S3

deploy s3:
    stage: deploy
    image:
        name: amazon/aws-cli:2.4.11
        entrypoint: [""]
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    script:
        - aws --version
        - aws s3 cp test.txt s3://$AZUCH_S3_BUCKET -> copy file
        - aws s3 sync build s3://$AZUCH_S3_BUCKET --delete -> sync build folder to s3 and delete non-sync file

5. Variables on GitLab env

- Protect: Only protect brach can reach these vars.
- Mask: It is hidden lke this: *****

6. Read the settings of AWS S3

7. It does't make sense when in dev mode, you keep always upload to s3, it eats all the bandwidth; beside, you just want to test the error when build web. So, just deploy to s3 when it merges to main.
And, we will use predefine variables of Gitlab.
https://docs.gitlab.com/ee/ci/variables/predefined_variables.html


rules:
    - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH

8. Adding post deploy test and it is only run when merge. Just check after done all the work, the web still alive.

variables:
    APP_BASE_URL: http://azuch-0942022.s3-website-us-east-1.amazonaws.com

post deploy test:
    stage: post test
    rules:
        - if: $CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH
    image: curlimages/curl
    script:
        - echo $APP_BASE_URL
        - curl $APP_BASE_URL | grep "React App" 

9. Adding staging environment before production environment.

S3

- Create new bucket: ...-staging
- ...

GitLab

- create envs: staging with url of s3-staging | production with url of s3
- change the variables: Setting - CI/CD - Variables
    + AWS_S3_BUCKET

- add stage: deploy staging & test
- in job, add: environment: staging

10. Reuse code: using .deploy and extends: .deploy

.deploy:
    ...

deploy staging:
    stage: deploy staging & test
    environment: staging
    extends: .deploy

11. Assign version using CI_PIPELINE_IID and test with it (ID is the Id, IID is the internal, it is number ascending)

12. Continuous Delivery using when: manual in deploy production

13. Create a dockerfile that do:
FROM nginx:1.21.6-alpine
COPY build /usr/share/nginx/html

14. We use BeanStalk now, so we gonna remove the deploy to s3 states

15. Add new state: build docker image
https://hub.docker.com/_/docker

build docker image:
    stage: package
    image: docker:20.10.14
    services:
        - docker:20.10.14-dind
    script:
        - docker build -t $CI_REGISTRY_IMAGE -t $CI_REGISTRY_IMAGE:$APP_VERSION .
        - docker image ls

16. After job finish, images will gone, so we will use gitlab's registry container to store registry of docker images.

Packages & registries - Container registries

17. Docker login to registry using -u (user'registry), CI_REGISTRY, and --password-stdin from echo $CI_REGISTRY_PASSWORD

echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY

18. We will test the docker image just build by checking the version after 

curl http://website/version.html | grep $APP_VERSION

19. Now, deploy to production

- first, add stage:

    deploy production:
    stage: deploy
    environment: production
    image:
        name: amazon/aws-cli:2.4.11
        entrypoint: [""]
    script:
        - aws --version
        - yum install -y gettext
        - envsubst < templates/Dockerrun.aws.json > Dockerrun.aws.json
        - envsubst < templates/auth.json > auth.json
        - cat *.json
        - aws s3 cp Dockerrun.aws.json s3://$AWS_S3_BUCKET
        - aws s3 cp auth.json s3://$AWS_S3_BUCKET

- Second, change the AWS_S3_BUCKET variables

- Third, add Deploy Token: Setting - repositories - deploy tokens With read repositories and read registry

- Fourth, add DEPLOY_TOKEN variable with mask

- The yaml file:
deploy production:
    stage: deploy
    environment: production
    image:
        name: amazon/aws-cli:2.4.11
        entrypoint: [""]
    script:
        - aws --version
        - yum install -y gettext -> need to install to use envsubst
        - export DEPLOY_TOKEN=$(echo $GITLAB_DEPLOY_TOKEN | tr -d "\n" | base64) -> aws required base64 decode
        - echo $AWS_S3_BUCKET
        - envsubst < templates/Dockerrun.aws.json > Dockerrun.aws.json -> create file
        - envsubst < templates/auth.json > auth.json -> create file
        - cat *.json
        - aws s3 cp Dockerrun.aws.json s3://$AWS_S3_BUCKET
        - aws s3 cp auth.json s3://$AWS_S3_BUCKET

